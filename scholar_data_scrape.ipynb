{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe7d2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import urllib\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00328971",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = pd.DataFrame(columns=[\"PMID\", \"TI\", \"PT\", \"JT\", \"DP\", \"OT\", \"MH\", \"RN\"])\n",
    "site_df = pd.DataFrame(columns=['PMID','AUTHOR','UNIVERSITY','DEPARTMENT','CITY','COUNTRY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e79f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    count = 0\n",
    "    soup = None\n",
    "    while soup is None:\n",
    "        try:\n",
    "            page = requests.get(\"https://pubmed.ncbi.nlm.nih.gov/?term=\" + url, allow_redirects=False, timeout=100)\n",
    "            soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "        except Exception as e: print(\"get_soup: \", e) \n",
    "        count += 1\n",
    "        if (count >= 20): break\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e6399d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def parse_location(string):   \n",
    "    university, department, city, country = None, None, None, None \n",
    "    string = string.replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    if \" - \" in string:\n",
    "        string = string.split(\" - \")[0]\n",
    "    components = string.strip().split(',')\n",
    "    components = list(OrderedDict.fromkeys(components))\n",
    "    components = [item for item in components if not any(char.isdigit() for char in item)]\n",
    "    if len(components) == 1:\n",
    "        university = components[0]\n",
    "    elif len(components) == 2:\n",
    "        university = components[0]\n",
    "        country = components[1]\n",
    "    elif len(components) == 3:\n",
    "        university = components[0]\n",
    "        city = components[1]\n",
    "        country = components[2]\n",
    "    elif len(components) == 4:\n",
    "        department = components[0]\n",
    "        university = components[1]\n",
    "        city = components[2]\n",
    "        country = components[3]\n",
    "    elif len(components) > 4:\n",
    "        university = components[0]\n",
    "        city = components[len(components)-2]\n",
    "        country = components[len(components)-1]\n",
    "\n",
    "    return university, department, city, country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40838840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_df(records):\n",
    "    paper = []\n",
    "    site = []\n",
    "    desired_fields = [\"PMID\", \"TI\", \"PT\", \"JT\", \"DP\", \"OT\", \"MH\", \"RN\"]\n",
    "    key = \"\"\n",
    "    value = \"\"\n",
    "    for record in records[:]:\n",
    "        entry = {}\n",
    "        lines = record.strip().split('\\n')\n",
    "        pmid = lines[0].strip()\n",
    "        entry['PMID'] = pmid\n",
    "        for line in lines[1:]:\n",
    "            parts = re.split(r'\\s*-\\s*', line, maxsplit=1)\n",
    "            if(len(parts) == 2):\n",
    "                key, value = parts\n",
    "                value = value.strip()\n",
    "                if key in desired_fields:\n",
    "                    try:\n",
    "                        if (entry[key]):\n",
    "                            entry[key] = entry[key] + \" / \" + value\n",
    "                    except:\n",
    "                        entry[key] = value\n",
    "            else:\n",
    "                if key in desired_fields:\n",
    "                    line = line.strip()\n",
    "                    entry[key] = entry[key] + line\n",
    "        paper.append(entry)\n",
    "        authors = re.split(r'FAU - ', record)\n",
    "        for author in authors:\n",
    "            entry = {}   \n",
    "            ad_match = re.findall(r'AD  - (.+)', author, flags=re.DOTALL)\n",
    "            if ad_match:\n",
    "                line = author.strip().split('\\n')\n",
    "                fau = line[0].strip()\n",
    "                ad = ' / '.join(ad_match)\n",
    "                university, department, city, country = parse_location(ad)\n",
    "                entry['PMID'] = pmid\n",
    "                entry['AUTHOR'] = fau\n",
    "                entry['UNIVERSITY'] = ad\n",
    "                entry['DEPARTMENT'] = ad\n",
    "                entry['CITY'] = ad\n",
    "                entry['COUNTRY'] = ad\n",
    "                site.append(entry)\n",
    "        records.remove(record)\n",
    "\n",
    "    paper_df_temp = pd.DataFrame(paper)\n",
    "    site_df_temp = pd.DataFrame(site)\n",
    "    return paper_df_temp, site_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "febc25ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01 2023-12-01 2023-11-01 2023-10-01 2023-09-01 2023-08-01 2023-07-01 2023-06-01 2023-05-01 2023-04-01 2023-03-01 2023-02-01 2023-01-01 2022-12-01 2022-11-01 2022-10-01 2022-09-01 2022-08-01 2022-07-01 2022-06-01 2022-05-01 2022-04-01 2022-03-01 2022-02-01 2022-01-01 2021-12-01 2021-11-01 2021-10-01 2021-09-01 2021-08-01 2021-07-01 2021-06-01 2021-05-01 2021-04-01 2021-03-01 2021-02-01 2021-01-01 2020-12-01 2020-11-01 2020-10-01 2020-09-01 2020-08-01 2020-07-01 2020-06-01 2020-05-01 2020-04-01 2020-03-01 2020-02-01 2020-01-01 2019-12-01 2019-11-01 2019-10-01 2019-09-01 2019-08-01 2019-07-01 2019-06-01 2019-05-01 2019-04-01 2019-03-01 2019-02-01 2019-01-01 2018-12-01 2018-11-01 2018-10-01 2018-09-01 2018-08-01 2018-07-01 2018-06-01 2018-05-01 2018-04-01 2018-03-01 2018-02-01 2018-01-01 2017-12-01 2017-11-01 2017-10-01 2017-09-01 2017-08-01 2017-07-01 2017-06-01 2017-05-01 2017-04-01 2017-03-01 2017-02-01 2017-01-01 2016-12-01 2016-11-01 2016-10-01 2016-09-01 2016-08-01 2016-07-01 2016-06-01 2016-05-01 2016-04-01 2016-03-01 2016-02-01 2016-01-01 2015-12-01 2015-11-01 2015-10-01 2015-09-01 2015-08-01 2015-07-01 2015-06-01 2015-05-01 2015-04-01 2015-03-01 2015-02-01 2015-01-01 2014-12-01 2014-11-01 2014-10-01 2014-09-01 2014-08-01 2014-07-01 2014-06-01 2014-05-01 2014-04-01 2014-03-01 2014-02-01 2014-01-01 2013-12-01 2013-11-01 2013-10-01 2013-09-01 2013-08-01 2013-07-01 2013-06-01 2013-05-01 2013-04-01 2013-03-01 2013-02-01 2013-01-01 2012-12-01 2012-11-01 2012-10-01 2012-09-01 2012-08-01 2012-07-01 2012-06-01 2012-05-01 2012-04-01 2012-03-01 2012-02-01 2012-01-01 2011-12-01 2011-11-01 2011-10-01 2011-09-01 2011-08-01 2011-07-01 2011-06-01 2011-05-01 2011-04-01 2011-03-01 2011-02-01 2011-01-01 2010-12-01 2010-11-01 2010-10-01 2010-09-01 2010-08-01 2010-07-01 2010-06-01 2010-05-01 2010-04-01 2010-03-01 2010-02-01 2010-01-01 2009-12-01 2009-11-01 2009-10-01 2009-09-01 2009-08-01 2009-07-01 2009-06-01 2009-05-01 2009-04-01 2009-03-01 2009-02-01 2009-01-01 2008-12-01 2008-11-01 2008-10-01 2008-09-01 2008-08-01 2008-07-01 2008-06-01 2008-05-01 2008-04-01 2008-03-01 2008-02-01 2008-01-01 2007-12-01 2007-11-01 2007-10-01 2007-09-01 2007-08-01 2007-07-01 2007-06-01 2007-05-01 2007-04-01 2007-03-01 2007-02-01 2007-01-01 2006-12-01 2006-11-01 2006-10-01 2006-09-01 2006-08-01 2006-07-01 2006-06-01 2006-05-01 2006-04-01 2006-03-01 2006-02-01 2006-01-01 2005-12-01 2005-11-01 2005-10-01 2005-09-01 2005-08-01 2005-07-01 2005-06-01 2005-05-01 2005-04-01 2005-03-01 2005-02-01 2005-01-01 2004-12-01 2004-11-01 2004-10-01 2004-09-01 2004-08-01 2004-07-01 2004-06-01 2004-05-01 2004-04-01 2004-03-01 2004-02-01 2004-01-01 2003-12-01 2003-11-01 2003-10-01 2003-09-01 2003-08-01 2003-07-01 2003-06-01 2003-05-01 2003-04-01 2003-03-01 2003-02-01 2003-01-01 2002-12-01 2002-11-01 2002-10-01 2002-09-01 2002-08-01 2002-07-01 2002-06-01 2002-05-01 2002-04-01 2002-03-01 2002-02-01 2002-01-01 2001-12-01 2001-11-01 2001-10-01 2001-09-01 2001-08-01 2001-07-01 2001-06-01 2001-05-01 2001-04-01 2001-03-01 2001-02-01 2001-01-01 2000-12-01 2000-11-01 2000-10-01 2000-09-01 2000-08-01 2000-07-01 2000-06-01 2000-05-01 2000-04-01 2000-03-01 2000-02-01 2000-01-01 1999-12-01 1999-11-01 1999-10-01 1999-09-01 1999-08-01 1999-07-01 1999-06-01 1999-05-01 1999-04-01 1999-03-01 1999-02-01 1999-01-01 1998-12-01 1998-11-01 1998-10-01 1998-09-01 1998-08-01 1998-07-01 1998-06-01 1998-05-01 1998-04-01 1998-03-01 1998-02-01 1998-01-01 1997-12-01 1997-11-01 1997-10-01 1997-09-01 1997-08-01 1997-07-01 1997-06-01 1997-05-01 1997-04-01 1997-03-01 1997-02-01 1997-01-01 1996-12-01 1996-11-01 1996-10-01 1996-09-01 1996-08-01 1996-07-01 1996-06-01 1996-05-01 1996-04-01 1996-03-01 1996-02-01 1996-01-01 1995-12-01 1995-11-01 1995-10-01 1995-09-01 1995-08-01 1995-07-01 1995-06-01 1995-05-01 1995-04-01 1995-03-01 1995-02-01 1995-01-01 1994-12-01 1994-11-01 1994-10-01 1994-09-01 1994-08-01 1994-07-01 1994-06-01 1994-05-01 1994-04-01 1994-03-01 1994-02-01 1994-01-01 1993-12-01 1993-11-01 1993-10-01 1993-09-01 1993-08-01 1993-07-01 1993-06-01 1993-05-01 1993-04-01 1993-03-01 1993-02-01 1993-01-01 "
     ]
    }
   ],
   "source": [
    "leftovers = []\n",
    "i = 2\n",
    "r1 = 0\n",
    "r2 = 0\n",
    "try:\n",
    "\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    end_date = datetime(1993, 1, 1)\n",
    "    current_date = start_date\n",
    "    \n",
    "    while current_date >= end_date:\n",
    "        print(current_date.strftime(\"%Y-%m-%d\"), end = ' ')\n",
    "        current_date_next = current_date - relativedelta(months=+1)\n",
    "        \n",
    "        url1 = \"%28%28\"+str(current_date_next.year)+\"%2F\"+str(current_date_next.month)+\"%2F\"+str(current_date_next.day)+\"%5BDate+-+Create%5D+%3A+\"+str(current_date.year)+\"%2F\"+str(current_date.month)+\"%2F\"+str(current_date.day)+\"%5BDate+-+Create%5D%29%29+AND+%28alzheimer%29\"\n",
    "        \n",
    "        soup = get_soup(url1)\n",
    "        log_resultcount_tag  = soup.find('meta', {'name': 'log_resultcount'})\n",
    "        resultcount_value = log_resultcount_tag.get('content')\n",
    "        resultcount = (int(resultcount_value) // 200)+1\n",
    "        if (resultcount <= 50):\n",
    "            r1 = resultcount\n",
    "            r2 = 1\n",
    "        elif (resultcount <= 100):\n",
    "            r1 = 50\n",
    "            r2 = resultcount - 49\n",
    "        else: \n",
    "            print(\"******** problem on url: \", url)\n",
    "            leftovers.append(current_date)\n",
    "            \n",
    "        for x in range(1,r1+1):\n",
    "            url_end = \"&size=200&sort=fauth&sort_order=asc&format=pubmed&page=\"+str(x)\n",
    "            url= url1 + url_end\n",
    "            soup = get_soup(url)\n",
    "            temp_list = re.split(r'\\bPMID- ', soup.text)[1:]\n",
    "            paper_df_temp, site_df_temp = write_to_df(temp_list) \n",
    "            paper_df = pd.concat([paper_df, paper_df_temp])\n",
    "            site_df = pd.concat([site_df, site_df_temp])\n",
    "        for x in range(1,r2+1):\n",
    "            url_end = \"&size=200&sort=fauth&sort_order=desc&format=pubmed&page=\"+str(x)\n",
    "            url= url1 + url_end\n",
    "            soup = get_soup(url)\n",
    "            temp_list = re.split(r'\\bPMID- ', soup.text)[1:]\n",
    "            paper_df_temp, site_df_temp = write_to_df(temp_list) \n",
    "            paper_df = pd.concat([paper_df, paper_df_temp])\n",
    "            site_df = pd.concat([site_df, site_df_temp])\n",
    "            if(site_df.shape[0]> 3000000):\n",
    "                paper_csv = \"paper_\" + str(i)+\".csv\"\n",
    "                site_csv = \"site_\" + str(i)+\".csv\"\n",
    "                i = i + 1\n",
    "                paper_df.to_csv(paper_csv, sep=';')\n",
    "                site_df.to_csv(site_csv, sep=';')\n",
    "                paper_df = pd.DataFrame(columns=[\"PMID\", \"TI\", \"PT\", \"JT\", \"DP\", \"OT\", \"MH\", \"RN\"])\n",
    "                site_df = pd.DataFrame(columns=['PMID','AUTHOR','UNIVERSITY','DEPARTMENT','CITY','COUNTRY']) \n",
    "        current_date = current_date_next\n",
    "except Exception as e: \n",
    "    print(\"*\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31c6c4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>TI</th>\n",
       "      <th>PT</th>\n",
       "      <th>JT</th>\n",
       "      <th>DP</th>\n",
       "      <th>OT</th>\n",
       "      <th>MH</th>\n",
       "      <th>RN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38070385</td>\n",
       "      <td>New bithiophene derivative attenuated Alzheime...</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Journal of trace elements in medicine and biol...</td>\n",
       "      <td>2023 Dec 8</td>\n",
       "      <td>AChE / Antioxidants / GSK3-β / MAO / Neurotran...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38061270</td>\n",
       "      <td>Unraveling the role of miRNAs in the diagnosis...</td>\n",
       "      <td>Journal Article / Review</td>\n",
       "      <td>Pathology, research and practice</td>\n",
       "      <td>2023 Dec 4</td>\n",
       "      <td>Alzheimer’s disease / Diagnosis / Mental healt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38083415</td>\n",
       "      <td>Effect of Comorbidities Features in Machine Le...</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Annual International Conference of the IEEE En...</td>\n",
       "      <td>2023 Jul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38087743</td>\n",
       "      <td>Preoperative electroencephalographic alpha-pow...</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>British journal of anaesthesia</td>\n",
       "      <td>2023 Dec 11</td>\n",
       "      <td>alpha attenuation / attention / delirium / ele...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38053545</td>\n",
       "      <td>A glance through the effects of CD4(+) T cells...</td>\n",
       "      <td>Journal Article / Review</td>\n",
       "      <td>Computational and structural biotechnology jou...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Alzheimer's disease / Amyloid β-protein / CD4+...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>37961246</td>\n",
       "      <td>Human neural stem cells restore spatial memory...</td>\n",
       "      <td>Preprint</td>\n",
       "      <td>bioRxiv : the preprint server for biology</td>\n",
       "      <td>2023 Nov 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>37961679</td>\n",
       "      <td>Regional interneuron transcriptional changes r...</td>\n",
       "      <td>Preprint</td>\n",
       "      <td>bioRxiv : the preprint server for biology</td>\n",
       "      <td>2023 Nov 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>37939533</td>\n",
       "      <td>A comparative study of GNN and MLP based machi...</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Neural networks : the official journal of the ...</td>\n",
       "      <td>2023 Oct 26</td>\n",
       "      <td>Alzheimer’s Disease (AD) / Data fusion / Graph...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>37931808</td>\n",
       "      <td>Chronic pain accelerates cognitive impairment ...</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Brain research bulletin</td>\n",
       "      <td>2023 Dec</td>\n",
       "      <td>Alzheimer's disease / CCL2 / Chronic pain / Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>37992128</td>\n",
       "      <td>Energy-adjusted dietary inflammatory index and...</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>Nutritional neuroscience</td>\n",
       "      <td>2023 Nov 22</td>\n",
       "      <td>Alzheimer’s disease / E-DII / cognitive functi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1036 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PMID                                                 TI  \\\n",
       "0    38070385  New bithiophene derivative attenuated Alzheime...   \n",
       "1    38061270  Unraveling the role of miRNAs in the diagnosis...   \n",
       "2    38083415  Effect of Comorbidities Features in Machine Le...   \n",
       "3    38087743  Preoperative electroencephalographic alpha-pow...   \n",
       "4    38053545  A glance through the effects of CD4(+) T cells...   \n",
       "..        ...                                                ...   \n",
       "195  37961246  Human neural stem cells restore spatial memory...   \n",
       "196  37961679  Regional interneuron transcriptional changes r...   \n",
       "197  37939533  A comparative study of GNN and MLP based machi...   \n",
       "198  37931808  Chronic pain accelerates cognitive impairment ...   \n",
       "199  37992128  Energy-adjusted dietary inflammatory index and...   \n",
       "\n",
       "                           PT  \\\n",
       "0             Journal Article   \n",
       "1    Journal Article / Review   \n",
       "2             Journal Article   \n",
       "3             Journal Article   \n",
       "4    Journal Article / Review   \n",
       "..                        ...   \n",
       "195                  Preprint   \n",
       "196                  Preprint   \n",
       "197           Journal Article   \n",
       "198           Journal Article   \n",
       "199           Journal Article   \n",
       "\n",
       "                                                    JT           DP  \\\n",
       "0    Journal of trace elements in medicine and biol...   2023 Dec 8   \n",
       "1                     Pathology, research and practice   2023 Dec 4   \n",
       "2    Annual International Conference of the IEEE En...     2023 Jul   \n",
       "3                       British journal of anaesthesia  2023 Dec 11   \n",
       "4    Computational and structural biotechnology jou...         2023   \n",
       "..                                                 ...          ...   \n",
       "195          bioRxiv : the preprint server for biology   2023 Nov 4   \n",
       "196          bioRxiv : the preprint server for biology   2023 Nov 4   \n",
       "197  Neural networks : the official journal of the ...  2023 Oct 26   \n",
       "198                            Brain research bulletin     2023 Dec   \n",
       "199                           Nutritional neuroscience  2023 Nov 22   \n",
       "\n",
       "                                                    OT   MH   RN  \n",
       "0    AChE / Antioxidants / GSK3-β / MAO / Neurotran...  NaN  NaN  \n",
       "1    Alzheimer’s disease / Diagnosis / Mental healt...  NaN  NaN  \n",
       "2                                                  NaN  NaN  NaN  \n",
       "3    alpha attenuation / attention / delirium / ele...  NaN  NaN  \n",
       "4    Alzheimer's disease / Amyloid β-protein / CD4+...  NaN  NaN  \n",
       "..                                                 ...  ...  ...  \n",
       "195                                                NaN  NaN  NaN  \n",
       "196                                                NaN  NaN  NaN  \n",
       "197  Alzheimer’s Disease (AD) / Data fusion / Graph...  NaN  NaN  \n",
       "198  Alzheimer's disease / CCL2 / Chronic pain / Co...  NaN  NaN  \n",
       "199  Alzheimer’s disease / E-DII / cognitive functi...  NaN  NaN  \n",
       "\n",
       "[1036 rows x 8 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9ef7912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1217152, 6)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9cb6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df.to_csv(\"alzheimers_paper\", sep=';')\n",
    "site_df.to_csv(\"alzheimers_site\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5efb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
